---
apiVersion: v1
kind: Namespace
metadata:
  name: llms-mistral
  labels:
    pod-security.kubernetes.io/audit: privileged
    pod-security.kubernetes.io/warn: privileged
    pod-security.kubernetes.io/enforce: privileged
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mistral-7b
  namespace: llms-mistral
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi
  storageClassName: rook-cephfs
  volumeMode: Filesystem
---
apiVersion: v1
kind: Secret
metadata:
  name: hf-token-secret
  namespace: llms-mistral
type: Opaque
stringData:
  token: "REPLACE_WITH_TOKEN"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mistral-7b
  namespace: llms-mistral
  labels:
    app: mistral-7b
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mistral-7b
  template:
    metadata:
      labels:
        app: mistral-7b
    spec:
      nodeSelector:
        role: inference-node
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node-role.kubernetes.io/master
                    operator: DoesNotExist
                  - key: node-role.kubernetes.io/control-plane
                    operator: DoesNotExist
                  - key: role
                    operator: NotIn
                    values:
                      - storage-node
      volumes:
        - name: cache-volume
          persistentVolumeClaim:
            claimName: mistral-7b
        # vLLM needs to access the host's shared memory for tensor parallel inference.
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: "2Gi"
      containers:
        - name: mistral-7b
          image: vllm/vllm-openai:latest
          command: [ "/bin/sh", "-c" ]
          args: [
            "vllm serve mistralai/Mistral-7B-Instruct-v0.3 --trust-remote-code --enable-chunked-prefill --max_num_batched_tokens 1024"
          ]
          env:
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token-secret
                  key: token
          ports:
            - containerPort: 8000
          resources:
            limits:
              cpu: "8"
              memory: 20G
              nvidia.com/gpu: "1"
            requests:
              cpu: "4"
              memory: 6G
              nvidia.com/gpu: "1"
          volumeMounts:
            - mountPath: /root/.cache/huggingface
              name: cache-volume
            - name: shm
              mountPath: /dev/shm
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 60
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 60
            periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    purelb.io/allocated-by: PureLB
    purelb.io/allocated-from: default
  labels:
    app: mistral-7b
  name: mistral-7b
  namespace: llms-mistral
spec:
  allocateLoadBalancerNodePorts: true
  externalTrafficPolicy: Cluster
  internalTrafficPolicy: Cluster
  ipFamilies:
    - IPv4
  ipFamilyPolicy: SingleStack
  ports:
    - nodePort: 30471
      port: 8000
      protocol: TCP
      targetPort: 8000
  selector:
    app: mistral-7b
  sessionAffinity: None
  type: LoadBalancer
