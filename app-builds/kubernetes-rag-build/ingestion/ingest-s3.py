import os
import boto3
from botocore.client import Config
from qdrant_client import QdrantClient
from qdrant_client.http import models
from sentence_transformers import SentenceTransformer
import io

# Configuration
QDRANT_HOST = os.getenv("QDRANT_HOST", "qdrant.default.svc.cluster.local")
_port_env = os.getenv("QDRANT_PORT", "6333")
if "://" in _port_env:
    # Handle Kubernetes service env var like tcp://10.x.x.x:6333
    QDRANT_PORT = int(_port_env.split(":")[-1])
else:
    QDRANT_PORT = int(_port_env)
COLLECTION_NAME = "codebase"
CHUNK_SIZE = 1000

# S3 Configuration
S3_ENDPOINT = os.getenv("S3_ENDPOINT")
S3_ACCESS_KEY = os.getenv("AWS_ACCESS_KEY_ID")
S3_SECRET_KEY = os.getenv("AWS_SECRET_ACCESS_KEY")
BUCKET_NAME = os.getenv("BUCKET_NAME")

# Allowed extensions
ALLOWED_EXTENSIONS = os.getenv("ALLOWED_EXTENSIONS", ".md,.sh,.yaml,.yml,.py,.txt,.c,.h,.cpp,.hpp,.cs,.json").split(",")

def get_s3_client():
    return boto3.client(
        's3',
        endpoint_url=S3_ENDPOINT,
        aws_access_key_id=S3_ACCESS_KEY,
        aws_secret_access_key=S3_SECRET_KEY,
        config=Config(signature_version='s3v4'),
        region_name='us-east-1'
    )

def get_files_from_s3(s3_client):
    files = []
    extensions = tuple(ALLOWED_EXTENSIONS)
    paginator = s3_client.get_paginator('list_objects_v2')
    for page in paginator.paginate(Bucket=BUCKET_NAME):
        if 'Contents' in page:
            for obj in page['Contents']:
                key = obj['Key']
                if key.endswith(extensions):
                    files.append(key)
    return files

def chunk_text(text, size):
    return [text[i:i + size] for i in range(0, len(text), size)]

def ingest():
    client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
    s3_client = get_s3_client()
    model = SentenceTransformer('all-MiniLM-L6-v2')

    # Create collection if not exists
    collections = client.get_collections().collections
    exists = any(c.name == COLLECTION_NAME for c in collections)
    
    if not exists:
        client.recreate_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=models.VectorParams(size=384, distance=models.Distance.COSINE),
        )
        print(f"Created collection: {COLLECTION_NAME}")

    files = get_files_from_s3(s3_client)
    print(f"Found {len(files)} files in S3 to ingest.")

    points = []
    idx = 0
    for s3_key in files:
        try:
            response = s3_client.get_object(Bucket=BUCKET_NAME, Key=s3_key)
            content = response['Body'].read().decode('utf-8')
            chunks = chunk_text(content, CHUNK_SIZE)
            
            for i, chunk in enumerate(chunks):
                vector = model.encode(chunk).tolist()
                points.append(models.PointStruct(
                    id=idx,
                    vector=vector,
                    payload={
                        "path": s3_key,
                        "chunk": i,
                        "text": chunk
                    }
                ))
                idx += 1
                
                if len(points) >= 100:
                    client.upsert(collection_name=COLLECTION_NAME, points=points)
                    points = []
                    print(f"Ingested {idx} chunks...")
        except Exception as e:
            print(f"Error processing {s3_key}: {e}")

    if points:
        client.upsert(collection_name=COLLECTION_NAME, points=points)
    
    print(f"Finished! Ingested total {idx} chunks from S3.")

if __name__ == "__main__":
    ingest()
